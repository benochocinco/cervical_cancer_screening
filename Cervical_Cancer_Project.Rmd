---
title: "Predicting Cases of Cervical Cancer in the 'Hospital Universitario de Caracas'"
author: "Ben Herndon-Miller and Nicole Jaiyesimi"
date: "11/25/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      results = "hide")
```

```{r warning=FALSE, message=FALSE}
# Load dependencies
library(dplyr)
library(knitr)
library(caret)
library(DMwR)
library(purrr)
library(pROC)
library(mlbench)
library(ggplot2)

# Read in data
cancer_raw <- read.csv("risk_factors_cervical_cancer.csv", header = TRUE, stringsAsFactors = FALSE)
```

# Introduction

# Data Description

The data was collected at Hospital Universitario (which is located in Caracas Venezuela) in 2017. There are 36 attributes in the dataset, all of which are either integers or booleans (e.g. age, number of sexual partners, whether or not the individual smokes, whether or not the individual has AIDS). There are 858 instances of the 36 attributes, but there are many missing values; we will conduct a coverage analysis of the data in our final project. 

Furthermore, the description of the dataset does not include detailed information on a variety of variables. First and foremost, there are 4 variables that could be considered the outcome variable of interest for a diagnosis of cancer: *Hinselmann*, *Schiller*, *Citology*, or *Biopsy*. These all represent different screening techniques for identifying and diagnosing cervical cancer. Since we need to determine a single outcome variable to predict, we proceed using the binary variable *Biopsy* as it is the gold standard for diagnosing cervical cancer. We will discard the other potential outcome variables from our dataset before we proceed.

First, let us examine the number of missing values for each variable before we proceed.

## Coverage Analysis and Missing Data Methodology

```{r echo = FALSE}
# Remove other outcome variables
drops <- c("Hinselmann", "Schiller", "Citology")
cancer_df <- cancer_raw[,!names(cancer_raw) %in% drops]

# Create coverage data frame
data_coverage <- data.frame(matrix(nrow = ncol(cancer_df), ncol = 4))
colnames(data_coverage) <- c("Variable","Variable_Type", "Missing_Values", "Percent_Missing")
data_coverage$Variable <- names(cancer_df)
var_types <- c("int","int","int","int","bool","int","int","bool","int","bool","int","bool","int","bool","bool","bool","bool","bool","bool","bool","bool","bool","bool","bool","bool","int","int","int","bool","bool","bool","bool","bool")
data_coverage$Variable_Type <- var_types
data_coverage$Missing_Values <- colSums(cancer_df == "?")
data_coverage$Percent_Missing <- colSums(cancer_df == "?")/nrow(cancer_df)

# Replace ? with NA
cancer_df[cancer_df == "?"] <- NA

kable(data_coverage, format = "markdown")
```

We can see from this coverage analysis that the majority of our variables have missing data points. However, most features are not missing a high-proportion of values, except for Time Since First/Last Diagnosis of STD. These are all missing for the patients who stated that they had not had any STD's, so it would be inaccurate to simply replace them with 0 as that would bias the model towards more patients having a small time period since their STD. Replacing them with the mean or median value would also not make sense since the true value would be infinity since they have never had a diagnosis. For the sake of simplicity, we will move forward by removing these two variables from our analysis. 

However, we still must deal with the missing values for the other variables. For the integer variables of *Age of first sexual intercourse*, *Number of sexual partners*, and *Number of pregnancies*, we can replace the missing values with the median values of their respective variables. In contrast, for the variables pertaining to smoking, STD's, and contraceptives, we cannot simply replace the missing values with the median value as they are based on boolean values. Thus, we will then remove the rows with missing values for the missing variables and report.

```{r echo = FALSE}
# Drop time since first/last diagnosis variables as we don't have a good method for dealing with them
drops <- c(drops,"STDs..Time.since.first.diagnosis","STDs..Time.since.last.diagnosis")
cancer_df <- cancer_df[,!names(cancer_df) %in% drops]

# Convert all variables to numeric and factor from character since ?'s induced character variables
for (i in 1:ncol(cancer_df)){
  idx <- names(cancer_df)[i] == data_coverage$Variable
  type <- data_coverage$Variable_Type[idx]
  if (type == "int"){
    cancer_df[,i] <- as.numeric(cancer_df[,i])
  }
  if (type == "bool"){
    cancer_df[,i] <- as.factor(cancer_df[,i])
  }
}

# Get rid of boolean variables with less than 10 instances of each class
for (i in 1:ncol(cancer_df)){
  if (is.factor(cancer_df[,i])){
    if (sum(cancer_df[,i] == 1, na.rm = TRUE) < 10){
      drops <- c(drops,names(cancer_df)[i])
    }
  }
}


# Replace missing values with median values for specified variables
cancer_df$First.sexual.intercourse[is.na(cancer_df$First.sexual.intercourse)] <- median(cancer_df$First.sexual.intercourse, na.rm = TRUE)
cancer_df$Number.of.sexual.partners[is.na(cancer_df$Number.of.sexual.partners)] <- median(cancer_df$Number.of.sexual.partners, na.rm = TRUE)
cancer_df$Num.of.pregnancies[is.na(cancer_df$Num.of.pregnancies)] <- median(cancer_df$Num.of.pregnancies, na.rm = TRUE)

# Only keep rows without missing values
cancer_df <- cancer_df[complete.cases(cancer_df),]
```

After cleaning the data with the above methodology we are left with a dataset of 726 observations and 31 features (30 predictors). We can now move forward with exploratory analysis and predictive modelling.

# Exploratory Data Analysis

```{r echo = FALSE}

```

# Predictive Modelling

We know from the above exploratory analysis that the outcome variable is highly imbalanced with only 6.9% of Biopsy's resulting in a cervical cancer diagnosis. Thus, incorporating methodologies for handling imbalanced classes will be vital for the success of our modelling.

Furthermore, we will need to split our data into training/validation, query and test sets in order to properly tune & compare our models before reporting the final accuracy of our best model on the test set. 

```{r echo = FALSE}
# Drop variables that only have one factor level and redundant variables to reduce multicollinearity
drops <- c(drops, "STDs.vulvo.perineal.condylomatosis")
cancer_model_df <- cancer_df[,!names(cancer_df) %in% drops] %>%
  rename(Number_of_sexual_partners = Number.of.sexual.partners, 
         First_sexual_intercourse = First.sexual.intercourse, 
         Num_of_pregnancies = Num.of.pregnancies, 
         Smokes_years = Smokes..years., 
         Smokes_packs_year=Smokes..packs.year.,
         Hormonal_Contraceptives = Hormonal.Contraceptives, 
         Hormonal_Contraceptives_years = Hormonal.Contraceptives..years.,
         IUD_years = IUD..years.,
         STDs_number = STDs..number.,
         STDs_condylomatosis = STDs.condylomatosis,
         STDs_syphilis = STDs.syphilis,
         STDs_HIV = STDs.HIV,
         STDs_Number_of_diagnosis = STDs..Number.of.diagnosis,
         Dx_Cancer = Dx.Cancer,
         Dx_HPV = Dx.HPV
         )

# Fix factor levels for classification 
for (i in 1:ncol(cancer_model_df)){
  if (is.factor(cancer_model_df[,i])){
    cancer_model_df[,i] <- make.names(cancer_model_df[,i])
  }
}

# Set seed for reproducibility
set.seed(123)

# Split data into training and query/test
train_idx <- createDataPartition(cancer_model_df$Biopsy, p = .7, list = FALSE)

train_cancer <- cancer_model_df[train_idx,] 
train_cancer_x <- train_cancer[,-ncol(train_cancer)]
train_cancer_y <- train_cancer[,ncol(train_cancer)]

test_cancer <- cancer_model_df[-train_idx,]
test_cancer_x <- test_cancer[,-ncol(test_cancer)]
test_cancer_y <- test_cancer[,ncol(test_cancer)]

# Set up train control for all of our models
ctrl <- trainControl(method = "cv",
                     number = 5,
                     returnResamp="all",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     allowParallel = TRUE)

# Build custom AUC function to extract AUC from the caret model object
# Used code from http://dpmartin42.github.io/posts/r/imbalanced-classes-part-1
test_roc <- function(model, data) {
  roc(data$Biopsy,
      predict(model, data, type = "prob")[, "X1"])
}
```

## Binomial Regression with Different Link Functions 

### Logistic Regression

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Set seed for reproducibility
set.seed(123)

# Fit basic logistic regression model 
logistic_fit <- train(Biopsy ~ ., 
                      data = train_cancer,
                      method = "glm",
                      family = binomial,
                      preProc = c("center", "scale"))

# Record training and test AUC
logistic_train_auc <- logistic_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

logistic_test_auc <- logistic_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Probit Regression

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Set seed for reproducibility
set.seed(123)

# Fit basic logistic regression model 
probit_fit <- train(Biopsy ~ ., 
                    data = train_cancer,
                    method = "glm",
                    family = binomial(link = "probit"),
                    preProc = c("center", "scale"))

# Record training and test AUC
probit_train_auc <- probit_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

probit_test_auc <- probit_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Cauchit Regression

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Set seed for reproducibility
set.seed(123)

# Fit basic logistic regression model 
cauchit_fit <- train(Biopsy ~ ., 
                     data = train_cancer,
                     method = "glm",
                     family = binomial(link = "cauchit"), 
                     preProc = c("center", "scale"))

# Record training and test AUC
cauchit_train_auc <- cauchit_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

cauchit_test_auc <- cauchit_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Complementary Log-Log Regression

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Set seed for reproducibility
set.seed(123)

# Fit basic logistic regression model 
cloglog_fit <- train(Biopsy ~ ., 
                     data = train_cancer,
                     method = "glm",
                     family = binomial(link = "cloglog"),
                     preProc = c("center", "scale"))

# Record training and test AUC
cloglog_train_auc <- cloglog_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

cloglog_test_auc <- cloglog_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Training and Test AUC for Binomial Regression with Different Link Functions

```{r echo = FALSE}
method <- c("Logistic Regression", "Probit Regression", "Cauchit Regression", "Complementary Log-Log Regression")

train_auc <- c(logistic_train_auc, probit_train_auc, cauchit_train_auc, cloglog_train_auc)

test_auc <- c(logistic_test_auc, probit_test_auc, cauchit_test_auc, cloglog_test_auc)

bin_reg_results <- data.frame(Method = method, Train_AUC = train_auc, Test_AUC = test_auc)

kable(bin_reg_results, format = "markdown")
```

### Receiver Operating Characteristic (ROC) Curves for Binomial Regression with Different Link Functions

```{r echo = FALSE}
model_list <- list(Logit = logistic_fit,
                   Probit = probit_fit,
                   Cauchit = cauchit_fit,
                   Complementary_Log_Log = cloglog_fit)

model_list_train_roc <- model_list %>%
  map(test_roc, data = train_cancer)

model_list_test_roc <- model_list %>%
  map(test_roc, data = test_cancer)

col <- c("#000000", "#009E73", "#0072B2", "#D55E00")

results_list_train_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_train_roc){
  results_list_train_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_train_roc <- bind_rows(results_list_train_roc)

results_list_test_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_test_roc){
  results_list_test_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_test_roc <- bind_rows(results_list_test_roc)

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_train_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Training ROC Curves")

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_test_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Test ROC Curves")
```

## Logistic Regression with Different Penalties & Feature Selection Algorithms

### L1 (LASSO) Logistic Regression

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train L1 Logistic Regression to find optimal lambda
l1_logistic_fit <- train(Biopsy ~ ., 
                         data = train_cancer, 
                         method = "glmnet",
                         preProc = c("center", "scale"),
                         trControl = ctrl, 
                         metric = "ROC",
                         tuneGrid = expand.grid(alpha = 1,
                                                lambda = 10^seq(-1,-4,length=50)))

# Record training and test AUC
l1_logistic_train_auc <- l1_logistic_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

l1_logistic_test_auc <- l1_logistic_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### L2 (Ridge) Logistic Regression

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train L2 Logistic Regression to find optimal lambda
l2_logistic_fit <- train(Biopsy ~ ., 
                         data = train_cancer, 
                         method = "glmnet",
                         preProc = c("center", "scale"),
                         trControl = ctrl, 
                         metric = "ROC",
                         tuneGrid = expand.grid(alpha = 0,
                                                lambda = 10^seq(-1,-4,length=50)))

# Record training and test AUC
l2_logistic_train_auc <- l2_logistic_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

l2_logistic_test_auc <- l2_logistic_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Elastic Net Logistic Regression

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train Elastic Net Logistic Regression to find optimal lambda
elnet_logistic_fit <- train(Biopsy ~ ., 
                            data = train_cancer, 
                            method = "glmnet",
                            preProc = c("center", "scale"),
                            trControl = ctrl, 
                            metric = "ROC",
                            tuneGrid = expand.grid(alpha = seq(0.05,0.95,0.05),
                                                   lambda = 10^seq(-1,-4,length=50)))

# Record training and test AUC
elnet_logistic_train_auc <- elnet_logistic_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

elnet_logistic_test_auc <- elnet_logistic_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Stepwise Logistic Regression

```{r echo = FALSE, warning = FALSE, message = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train Stepwise Logistic Regression to find optimal features
stpwise_logistic_fit <- train(Biopsy ~ .,
                              data = train_cancer, 
                              method = "glmStepAIC",
                              preProc = c("center", "scale"),
                              trControl = ctrl,
                              metric = "ROC",
                              family = binomial)

# Record training and test AUC
stpwise_logistic_train_auc <- stpwise_logistic_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

stpwise_logistic_test_auc <- stpwise_logistic_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### Training and Test AUC for Logistic Regression with Penalties & Feature Selection

```{r echo = FALSE}
method <- c("Logistic Regression", "L1 Logistic Regression", "L2 Logistic Regression", "Elastic Net", "Stepwise Logistic Regression")

train_auc <- c(logistic_train_auc, l1_logistic_train_auc, l2_logistic_train_auc, elnet_logistic_train_auc, stpwise_logistic_train_auc)

test_auc <- c(logistic_test_auc, l1_logistic_test_auc, l2_logistic_test_auc, elnet_logistic_test_auc, stpwise_logistic_test_auc)

log_reg_results <- data.frame(Method = method, Train_AUC = train_auc, Test_AUC = test_auc)

kable(log_reg_results, format = "markdown")
```

### Receiver Operating Characteristic (ROC) Curves for Logistic Regression with Penalties & Feature Selection

```{r echo = FALSE}
model_list <- list(logistic_regression = logistic_fit,
                   L1_logistic_regression = l1_logistic_fit,
                   L2_logistic_regression = l2_logistic_fit,
                   Elastic_net = elnet_logistic_fit,
                   Stepwise_Logistic_Regression = stpwise_logistic_fit)

model_list_train_roc <- model_list %>%
  map(test_roc, data = train_cancer)

model_list_test_roc <- model_list %>%
  map(test_roc, data = test_cancer)

col <- c("#000000", "#009E73", "#0072B2", "#D55E00", "#CC79A7")

results_list_train_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_train_roc){
  results_list_train_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_train_roc <- bind_rows(results_list_train_roc)

results_list_test_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_test_roc){
  results_list_test_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_test_roc <- bind_rows(results_list_test_roc)

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_train_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Training ROC Curves")

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_test_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Test ROC Curves")
```

## Penalized Logistic Regression with Re-sampling Techniques

Since the Elastic Net performed the best out of all the previous methods on the test set, we will proceed to re-tune the model with different re-sampling techniques to see if we can improve our performance. 

### Elastic Net Logistic Regression with Class Weights

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Create class weights
model_weights <- ifelse(train_cancer$Biopsy == "X0",
                        (1/table(train_cancer$Biopsy)[1]) * 0.5,
                        (1/table(train_cancer$Biopsy)[2]) * 0.5)

# Use same seed as before
ctrl$seeds <- elnet_logistic_fit$control$seeds

# Train Elastic Net Logistic Regression to find optimal lambda
elnet_weight_fit <- train(Biopsy ~ ., 
                          data = train_cancer, 
                          method = "glmnet",
                          weights = model_weights,
                          preProc = c("center", "scale"),
                          trControl = ctrl, 
                          metric = "ROC",
                          tuneGrid = expand.grid(alpha = seq(0.05,0.95,0.05),
                                                 lambda = 10^seq(-1,-4,length=50)))

# Record training and test AUC
elnet_weight_train_auc <- elnet_weight_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

elnet_weight_test_auc <- elnet_weight_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Elastic Net Logistic Regression with Down-Sampling

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Initialize down-sampling
ctrl$sampling <- "down"

# Train Elastic Net Logistic Regression to find optimal lambda
elnet_down_fit <- train(Biopsy ~ ., 
                        data = train_cancer, 
                        method = "glmnet",
                        preProc = c("center", "scale"),
                        trControl = ctrl, 
                        metric = "ROC",
                        tuneGrid = expand.grid(alpha = seq(0.05,0.95,0.05),
                                               lambda = 10^seq(-1,-4,length=50)))

# Record training and test AUC
elnet_down_train_auc <- elnet_down_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

elnet_down_test_auc <- elnet_down_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Elastic Net Logistic Regression with Up-Sampling

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Initialize down-sampling
ctrl$sampling <- "up"

# Train Elastic Net Logistic Regression to find optimal lambda
elnet_up_fit <- train(Biopsy ~ ., 
                      data = train_cancer, 
                      method = "glmnet",
                      preProc = c("center", "scale"),
                      trControl = ctrl, 
                      metric = "ROC",
                      tuneGrid = expand.grid(alpha = seq(0.05,0.95,0.05),
                                             lambda = 10^seq(-1,-4,length=50)))

# Record training and test AUC
elnet_up_train_auc <- elnet_up_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

elnet_up_test_auc <- elnet_up_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Elastic Net Logistic Regression with SMOTE (Synthetic Minority Oversampling TEchnique)

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Initialize down-sampling
ctrl$sampling <- "smote"

# Train Elastic Net Logistic Regression to find optimal lambda
elnet_smote_fit <- train(Biopsy ~ ., 
                         data = train_cancer, 
                         method = "glmnet",
                         preProc = c("center", "scale"),
                         trControl = ctrl, 
                         metric = "ROC",
                         tuneGrid = expand.grid(alpha = seq(0.05,0.95,0.05),
                                                lambda = 10^seq(-1,-4,length=50)))

# Record training and test AUC
elnet_smote_train_auc <- elnet_smote_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

elnet_smote_test_auc <- elnet_smote_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Training and Test AUC for Elastic Net with Re-sampling Techniques

```{r echo = FALSE}
method <- c("Logistic Regression (Baseline)","No Re-sampling", "Class Weights", "Down-Sampling", "Up-Sampling", "SMOTE")

train_auc <- c(logistic_train_auc, elnet_logistic_train_auc, elnet_weight_train_auc, elnet_down_train_auc, elnet_up_train_auc, elnet_smote_train_auc)

test_auc <- c(logistic_test_auc, elnet_logistic_test_auc, elnet_weight_test_auc, elnet_down_test_auc, elnet_up_test_auc, elnet_smote_test_auc)

elnet_resamp_results <- data.frame(Method = method, Train_AUC = train_auc, Test_AUC = test_auc)

kable(elnet_resamp_results, format = "markdown")
```

### Receiver Operating Characteristic (ROC) Curves for Elastic Net with Re-sampling Techniques

```{r echo = FALSE}
model_list <- list(Logistic_Regression = logistic_fit,
                   No_resampling = elnet_logistic_fit,
                   Class_Weights = elnet_weight_fit,
                   Down_Sampling = elnet_down_fit,
                   Up_Sampling = elnet_up_fit,
                   SMOTE = elnet_smote_fit)

model_list_train_roc <- model_list %>%
  map(test_roc, data = train_cancer)

model_list_test_roc <- model_list %>%
  map(test_roc, data = test_cancer)

col <- c("#000000", "#009E73", "#0072B2", "#D55E00", "#CC79A7", "#FF4500")

results_list_train_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_train_roc){
  results_list_train_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_train_roc <- bind_rows(results_list_train_roc)

results_list_test_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_test_roc){
  results_list_test_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_test_roc <- bind_rows(results_list_test_roc)

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_train_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Training ROC Curves")

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_test_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Test ROC Curves")
```

## Other GLM Frameworks and SVM's

### Boosted Logistic Regression

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Reset sampling method
ctrl$sampling <- NULL

# Train Boosted Logistic Regression to find optimal number of bags
boosted_logistic_fit <- train(Biopsy ~ ., 
                             data = train_cancer, 
                             method = "glmboost",
                             preProc = c("center", "scale"),
                             trControl = ctrl, 
                             metric = "ROC",
                             tuneGrid = expand.grid(mstop = seq(10,200,10),
                                                    prune = "no"))

# Record training and test AUC
boosted_logistic_train_auc <- boosted_logistic_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

boosted_logistic_test_auc <- boosted_logistic_fit %>%
  test_roc(data = test_cancer) %>%
  auc()

```

### Generalized Additive Model

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train boosted Generalized Additive Model
gam_fit <- train(Biopsy ~ ., 
                 data = train_cancer, 
                 method = "gam",
                 preProc = c("center", "scale"),
                 trControl = ctrl, 
                 metric = "ROC")

# Record training and test AUC
gam_train_auc <- gam_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

gam_test_auc <- gam_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### Support Vector Machine with Linear Kernel

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train boosted Support Vector Machine with Linear Kernel to find optimal parameters
svm_linear_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "svmLinear",
                       preProc = c("center", "scale"),
                       trControl = ctrl, 
                       metric = "ROC")

# Record training and test AUC
svm_linear_train_auc <- svm_linear_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

svm_linear_test_auc <- svm_linear_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### Support Vector Machine with Radial Basis Kernel

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train boosted Support Vector Machine with Radial Basis Kernel to find optimal parameters
svm_radial_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "svmRadial",
                       preProc = c("center", "scale"),
                       trControl = ctrl, 
                       metric = "ROC")

# Record training and test AUC
svm_radial_train_auc <- svm_radial_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

svm_radial_test_auc <- svm_radial_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```


### Training and Test AUC for Other GLM Frameworks and SVM's

```{r echo = FALSE}
method <- c("Logistic Regression", "Boosted Logistic Regression", "Generalized Additive Model", "Linear SVM", "Radial Basis SVM")

train_auc <- c(logistic_train_auc, boosted_logistic_train_auc, gam_train_auc, svm_linear_train_auc, svm_radial_train_auc)

test_auc <- c(logistic_test_auc, boosted_logistic_test_auc, gam_test_auc, svm_linear_test_auc, svm_radial_test_auc)

glm_svm_results <- data.frame(Method = method, Train_AUC = train_auc, Test_AUC = test_auc)

kable(glm_svm_results, format = "markdown")
```

### Receiver Operating Characteristic (ROC) Curves for Other GLM Frameworks and SVM's

```{r echo = FALSE}
model_list <- list(Logistic_Regression = logistic_fit,
                   Boosted_Logistic_Regression = boosted_logistic_fit,
                   Generalized_Additive_Model = gam_fit,
                   Linear_SVM = svm_linear_fit,
                   Radial_Basis_SVM = svm_radial_fit)

model_list_train_roc <- model_list %>%
  map(test_roc, data = train_cancer)

model_list_test_roc <- model_list %>%
  map(test_roc, data = test_cancer)

col <- c("#000000", "#009E73", "#0072B2", "#D55E00", "#CC79A7")

results_list_train_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_train_roc){
  results_list_train_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_train_roc <- bind_rows(results_list_train_roc)

results_list_test_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_test_roc){
  results_list_test_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_test_roc <- bind_rows(results_list_test_roc)

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_train_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Training ROC Curves")

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_test_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Test ROC Curves")
```

## Modern Machine Learning Classifiers

### Random Forest

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train Random Forest
rf_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "rf",
                       preProc = c("center", "scale"),
                       trControl = ctrl, 
                       metric = "ROC")

# Record training and test AUC
rf_train_auc <- rf_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

rf_test_auc <- rf_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### Adaboost

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train Adaboost model
adaboost_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "adaboost",
                       preProc = c("center", "scale"),
                       trControl = ctrl, 
                       metric = "ROC")

# Record training and test AUC
adaboost_train_auc <- adaboost_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

adaboost_test_auc <- adaboost_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### Gradient Boosting Machine

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train Adaboost model
gbm_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "gbm",
                       preProc = c("center", "scale"),
                       trControl = ctrl, 
                       metric = "ROC")

# Record training and test AUC
gbm_train_auc <- gbm_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

gbm_test_auc <- gbm_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### eXtreme Gradient Boosting

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Train Xgboost model
xgb_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "xgbTree",
                       preProc = c("center", "scale"),
                       trControl = ctrl, 
                       metric = "ROC")

# Record training and test AUC
xgb_train_auc <- xgb_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

xgb_test_auc <- xgb_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### Training and Test AUC for Modern Machine Learning Classifiers

```{r echo = FALSE}
method <- c("Logistic Regression", "Random Forest", "Adaboost", "Gradient Boosting Machine", "eXtreme Gradient Boosting")

train_auc <- c(logistic_train_auc, rf_train_auc, adaboost_train_auc, gbm_train_auc, xgb_train_auc)

test_auc <- c(logistic_test_auc, rf_test_auc, adaboost_test_auc, gbm_test_auc, xgb_test_auc)

modern_ml_results <- data.frame(Method = method, Train_AUC = train_auc, Test_AUC = test_auc)

kable(modern_ml_results, format = "markdown")
```


### Receiver Operating Characteristic (ROC) Curves for Modern Machine Learning Classifiers

```{r echo = FALSE}
model_list <- list(Logistic_Regression = logistic_fit,
                   Random_Forest = rf_fit,
                   Adaboost = adaboost_fit,
                   GBM = gbm_fit,
                   xGB = xgb_fit)

model_list_train_roc <- model_list %>%
  map(test_roc, data = train_cancer)

model_list_test_roc <- model_list %>%
  map(test_roc, data = test_cancer)

col <- c("#000000", "#009E73", "#0072B2", "#D55E00", "#CC79A7")

results_list_train_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_train_roc){
  results_list_train_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_train_roc <- bind_rows(results_list_train_roc)

results_list_test_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_test_roc){
  results_list_test_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_test_roc <- bind_rows(results_list_test_roc)

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_train_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Training ROC Curves")

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_test_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Test ROC Curves")
```

## eXtreme Gradient Boosting with Re-sampling Techniques

Now that we have found our best classifier after train *a lot* of models. We can re-visit the re-sampling techniques we made use of earlier with the Elastic Net. 

### eXtreme Gradient Boosting with Down-Sampling

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Build down-sampled model
ctrl$sampling <- "down"

# Train Xgboost model
xgb_down_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "xgbTree",
                       preProc = c("center", "scale"),
                       trControl = ctrl,
                       metric = "ROC")

# Record training and test AUC
xgb_down_train_auc <- xgb_down_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

xgb_down_test_auc <- xgb_down_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### eXtreme Gradient Boosting with Up-Sampling

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Build down-sampled model
ctrl$sampling <- "up"

# Train Xgboost model
xgb_up_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "xgbTree",
                       preProc = c("center", "scale"),
                       trControl = ctrl,
                       metric = "ROC")

# Record training and test AUC
xgb_up_train_auc <- xgb_up_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

xgb_up_test_auc <- xgb_up_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### eXtreme Gradient Boosting with SMOTE (Synthetic Minority Oversampling TEchnique)

```{r echo = FALSE}
# Set seed for reproducibility
set.seed(123)

# Build down-sampled model
ctrl$sampling <- "smote"

# Train Xgboost model
xgb_smote_fit <- train(Biopsy ~ ., 
                       data = train_cancer, 
                       method = "xgbTree",
                       preProc = c("center", "scale"),
                       trControl = ctrl,
                       metric = "ROC")

# Record training and test AUC
xgb_smote_train_auc <- xgb_smote_fit %>%
  test_roc(data = train_cancer) %>%
  auc()

xgb_smote_test_auc <- xgb_smote_fit %>%
  test_roc(data = test_cancer) %>%
  auc()
```

### Training and Test AUC for eXtreme Gradient Boosting with Re-sampling Techniques

```{r echo = FALSE}
method <- c("Logistic Regression", "xGB", "xGB Down-Sampling", "xGB Up-Sampling", "xGB SMOTE")

train_auc <- c(logistic_train_auc, xgb_train_auc, xgb_down_train_auc, xgb_up_train_auc, xgb_smote_train_auc)

test_auc <- c(logistic_test_auc, xgb_test_auc, xgb_down_test_auc, xgb_up_test_auc, xgb_smote_test_auc)

xgb_samp_results <- data.frame(Method = method, Train_AUC = train_auc, Test_AUC = test_auc)

kable(xgb_samp_results, format = "markdown")
```

### Receiver Operating Characteristic (ROC) Curves for eXtreme Gradient Boosting with Re-sampling Techniques

```{r echo = FALSE}
model_list <- list(Logistic_Regression = logistic_fit,
                   xGB = xgb_fit,
                   xGB_Down = xgb_down_fit,
                   xGB_Up = xgb_up_fit,
                   xGB_SMOTE = xgb_smote_fit)

model_list_train_roc <- model_list %>%
  map(test_roc, data = train_cancer)

model_list_test_roc <- model_list %>%
  map(test_roc, data = test_cancer)

col <- c("#000000", "#009E73", "#0072B2", "#D55E00", "#CC79A7")

results_list_train_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_train_roc){
  results_list_train_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_train_roc <- bind_rows(results_list_train_roc)

results_list_test_roc <- list(NA)
num_mod <- 1

for(the_roc in model_list_test_roc){
  results_list_test_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  num_mod <- num_mod + 1
}

results_df_test_roc <- bind_rows(results_list_test_roc)

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_train_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Training ROC Curves")

ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_test_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18) +
  ggtitle("Test ROC Curves")
```

# Feature Importance

We can now look at feature importance for our optimal model to understand what is the best predictor of cervical cancer.

```{r echo = FALSE}
xgb_imp <- varImp(xgb_down_fit)

kable(xgb_imp$importance, format = 'markdown')
```

